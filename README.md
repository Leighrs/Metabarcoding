# Metabarcoding Directory Setup & Pipeline

![GitHub Repo Size](https://img.shields.io/github/repo-size/Leighrs/Metabarcoding)
![License](https://img.shields.io/github/license/Leighrs/Metabarcoding)
![Last Commit](https://img.shields.io/github/last-commit/Leighrs/Metabarcoding)

Welcome to the **Metabarcoding** repository for the **Genomic Variation Laboratory**!  
This repository helps lab members quickly set up a standardized directory structure on the HPC for running metabarcoding analyses using the **nf-core/ampliseq pipeline**.

---
## Table of Contents

- [Repository Overview](#repository-overview)
- [Current Files](#current-files)
- [Getting Started](#getting-started)
  - [1. Clone the Repository](#1-clone-the-repository)
  - [2. Set Up Your Project Directory on-hpc](#2-set-up-your-project-directory-on-hpc)
  - [3. Update the NCBI Database (Optional)](#3-update-the-ncbi-database-optional)
- [Running Pipeline](#running-pipeline)
  - [4. Run the nf-coreampliseq Pipeline](#4-run-the-nf-coreampliseq-pipeline)
  - [5. BLAST Unknown ASVs (Optional)](#5-blast-unknown-asvs-optional)
  - [6. Decontaminate ASVs and apply read thresholds (Optional)](#6-decontaminate-asvs-and-apply-read-thresholds)

## Repository Overview

This repository contains scripts and configuration files to:

1. Set up your project directory on the HPC.
2. Provide example files (samplesheets, metadata, and RSD sequences) for testing and reference.
3. Run the nf-core/ampliseq pipeline on your data.
4. Perform BLAST searches for unknown ASVs.
5. Update and download the NCBI core nucleotide database.
6. Clean and process ASV tables using downstream R scripts.

### Current Files

| File | Description |
|------|-------------|
| `nf-params.json` | Parameter file for the nf-core/ampliseq pipeline. Customize for your project. |
| `setup_metabarcoding_directory.sh` | Shell script to create your project directory with example samplesheets, metadata, and RSD files. |
| `update_blast_db.slurm` | SLURM batch script to download/update the NCBI core nucleotide database. |
| `blast_asv.slurm` | SLURM batch script to BLAST unknown ASVs. |
| `run_nf-core_ampliseq.slurm` | SLURM batch script to execute the nf-core/ampliseq pipeline. |
| `R_ASV_cleanup_scripts/` | Folder containing R scripts for cleaning and formatting ASV tables after nf-core/ampliseq and optional BLAST. |

> More scripts will be added over time to streamline additional steps.

---

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Leighrs/Metabarcoding.git
```
### 2. Set Up Your Project Directory on HPC
```bash
cd ~
./Metabarcoding/setup_metabarcoding_directory.sh
```
- Your project directory structure will look like:
```bash
Metabarcoding/
└── <project_name>/
    ├── input/
    │   ├── fastq/
    │   ├── Example_samplesheet.txt
    │   ├── Example_metadata.txt
    │   └── Example_RSD.txt
    ├── output/
    │   └── intermediates_logs_cache/
    │       └── singularity/
    └── scripts/ # If you need alter any scripts, edit these. I recommend leaving the originals alone in case you need to revert back to them.
        ├── <project_name>_ncbi_taxonomy.slurm
        ├── <project_name>_nf-params.json
        ├── <project_name>_ncbi_pipeline.py
        ├── <project_name>_blast_asv.slurm
        ├── <project_name>_generate_samplesheet_table.sh
        ├── <project_name>_run_nf-core_ampliseq.slurm
        ├── <project_name>_update_blast_db.slurm
        └── <project_name>_R_ASV_cleanup_scripts/
            ├── <project_name>_1_Data_Analyses_decontam_removal_251106.R
            ├── <project_name>_2_Data_Analyses_presence_absence_after_decontam_removal_251106.R
            ├── <project_name>_3_Data_Analyses_sample_threshold_251106.R
            ├── <project_name>_4_Data_Analyses_presence_absence_after_sample_threshold_251106.R
            ├── <project_name>_5_Data_Analyses_total_threshold_251106.R
            ├── <project_name>_6_Data_Analyses_presence_absence_after_total_threshold_251106.R
            └── <project_name>_GVL_metabarcoding_cleanup_main.R
current_project_name.txt # Do not edit this file. Other SLURM scripts need access to it.
Logs_archive/
```
### 3. Update the NCBI Database (Optional)
```bash
cd ~
PROJECT_NAME=$(cat "$HOME/Metabarcoding/current_project_name.txt")
sbatch "$HOME/Metabarcoding/$PROJECT_NAME/scripts/${PROJECT_NAME}_update_ncbi_db.sh" --delete-old --delete-old # Will delete old database. Recommended to save space in group folder. Only keep old folder if you need for something.
sbatch "$HOME/Metabarcoding/$PROJECT_NAME/scripts/${PROJECT_NAME}_update_ncbi_db.sh" # Will not delete old database.
```
## Running pipeline
### 4. Run the nf-core/ampliseq Pipeline
#### A. Import fastq files into $HOME/Metabarcoding/<project_name>/input/fastq/
        - If you are using a termianal, such as MobaXterm, you can drag and drop the files.
        - Other terminals may require you using code to transfer the files in. Online resources for this process can be found here: https://docs.hpc.ucdavis.edu/data-transfer/
        - If you are unsure of your <project_name>, run this code:
```bash
$HOME/Metabarcoding/current_project_name.txt
```   
#### B. Generate a samplesheet file.

```bash
cd ~
PROJECT_NAME=$(cat "$HOME/Metabarcoding/current_project_name.txt")
"$HOME/Metabarcoding/$PROJECT_NAME/scripts/${PROJECT_NAME}_generate_samplesheet_table.sh" 
```
- The script will autopopulate the PATHs for each of your fastq files, extrapolate sample names from those files, and prompt you to specify how many metabarcoding runs these samples were sequenced in.
- The script's default is to extrapolate sample names from the forward reads (R1) using the first two fields of the file name separated by and underscore ("_").
  - For example:
      B12A1_02_4_S14_L001_R1_001.fastq.gz  ->  B12A1_02
  - If you wish to extrapolate a different part of the file name, you can edit the following code chunk from the ${PROJECT_NAME}_generate_samplesheet_table.sh file:

```bash
#Replace this code chunk (lines 47-57) from the ${PROJECT_NAME}_generate_samplesheet_table.sh file:
PROJECT_NAME=$(cat "$HOME/Metabarcoding/current_project_name.txt")
nano $HOME/Metabarcoding/$PROJECT_NAME/scripts/${PROJECT_NAME}_generate_samplesheet_table.sh

extract_sample_id() {
    local filename="$1"
    
    # Remove R1/R2 etc. suffix from filename
    local base="${filename%_R1_001.fastq.gz}"

    # --- DEFAULT RULE ---
    # Extract the first TWO underscore-separated fields
    # e.g. B12A1_02_4_S14 ? B12A1_02
    echo "$base" | awk -F'_' '{print $1"_"$2}'
}

# with code specifying new extraploation rules. For example, this code specifies to only take the first part of file name as the sample ID. 
## For example: B12A1_02_4_S14_L001_R1_001.fastq.gz  ->  B12A1
### If you are unsure how to alter this code, please contact Leigh Sanders (lrsanders@ucdavis.edu)


extract_sample_id() {
    local filename="$1"
    
    # Remove R1/R2 etc. suffix from filename
    local base="${filename%_R1_001.fastq.gz}"

    # Extract ONLY the first underscore-separated field
    echo "$base" | awk -F'_' '{print $1}'
}

```

#### C. Upload Metadata to $HOME/Metabarcoding/<project_name>/input/
- There is an example metadata .txt file found in your project input folder.
- Rules:
  - Needs to be a tab-deliminated text file or a .tsv file.
  - First column is labeled "ID" for your sample IDs. These IDs match the sample IDs in your samplesheet you just made.
  - If you wish to use a decontamination protocol later, add a column called "Control_Assign" to assign which controls are paired with which samples.
    - For example:
                        sampleID         Control_Assign       Sample_or_Control
                          BROA1               1,2,4               Sample          <- Controls 1,2,4 need to be subtracted from this sample
                          FLYA2               2,3,4               Sample          <- Controls 2,3,4 need to be subtracted from this sample
                          BROAB               1                   Control         <- The ID of this control is 1
                          FLYAB               3                   Control         <- The ID of this control is 2
                          EXT1                2                   Control         <- The ID of this control is 3
                          PCR1                4                   Control         <- The ID of this control is 4
  - Add any other columns for metadata you wish to attach to these samples for downstream analyses.
- Drag and drop metadata file in, or use the directions here: https://docs.hpc.ucdavis.edu/data-transfer/

#### D. Upload a Reference Sequence Database to $HOME/Metabarcoding/<project_name>/input/ [Optional, but highly recommended]
- Rules:
- There is an example RSD .txt file found in your project input folder.
  - Needs to be a tab-deliminated text file or a .tsv file.
- Drag and drop RSD file in, or use the directions here: https://docs.hpc.ucdavis.edu/data-transfer/  

#### D. Edit run parameters:
```bash
PROJECT_NAME=$(cat "$HOME/Metabarcoding/current_project_name.txt")
nano $HOME/Metabarcoding/$PROJECT_NAME/scripts/${PROJECT_NAME}_nf-params.json
```
- The `${PROJECT_NAME}_nf-params.json` file contains all the parameters needed to run the `nf-core/ampliseq` workflow for your specific project. 
- Edit this file so that the input paths, primer sequences, and filtering settings match your dataset.
- Notes:
    - All paths *must be absolute (full paths), not environment variables or relative paths.
    - If you do want to set a parameter (e.g., `trunclenf`), use `null`. Leaving it blank will cause a JSON parsing error.
    - Booleans must be written without quotes:
      - `true` / `false` correct!
      - `"true"` / `"false"` invalid!
    - Primer sequences must include only the *target-specific* portion, not the adapters.



#### E. Run pipeline:
```bash
cd ~
PROJECT_NAME=$(cat "$HOME/Metabarcoding/current_project_name.txt")
sbatch "$HOME/Metabarcoding/$PROJECT_NAME/scripts/${PROJECT_NAME}_run_nf-core_ampliseq.slurm"
```
### 5. BLAST Unknown ASVs (Optional)
```bash
sbatch blast_asv.slurm
```
### 6. Decontaminate ASVs and apply read thresholds
- The folder `R_ASV_cleanup_scripts/` contains a collection of R scripts used for cleaning ASV data generated by the nf-core/ampliseq pipeline to prepare finalized datasets for analyses.
#### Available scripts include:
| Script | Description |
|------|-------------|
| `GVL_metabarcoding_cleanup_main.R` | Master script to run all ASV cleanup steps below. |
| `1_Data_Analyses_decontam_removal_251106.R` | Removes reads found in control samples from samples assigned to those controls. |
| `2_Data_Analyses_presence_absence_after_decontam_removal_251106.R` | Produces a presence/absence matrix after decontamination removal. |
| `3_Data_Analyses_sample_threshold_251106.R` | Applies an optional per-sample ASV abundance threshold. |
| `4_Data_Analyses_presence_absence_after_sample_threshold_251106.R` | Produces a presence/absence matrix after per-sample ASV abundance threshold. |
| `5_Data_Analyses_total_threshold_251106.R` | Applies a minimum sequencing depth threshold across all samples. |
| `6_Data_Analyses_presence_absence_after_total_threshold_251106.R` | Produces a final presence/absence matrix after minimum sequencing depth threshold. |

- These scripts are optional but extremely helpful for producing clean, analysis ready ASV tables.
- You should only need to open the `GVL_metabarcoding_cleanup_main.R` script to run this pipeline. To avoid breaking the script, only edit the **"User-defined parameters"** in the script.
